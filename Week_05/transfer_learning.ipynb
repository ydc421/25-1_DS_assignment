{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOlo4Ctmtm4p"
      },
      "source": [
        "# Transfer Learning Assignment\n",
        "반갑습니다 여러분. 과제를 맡은 김준호입니다. TL 세션에서 다룬 Fine-tuning과 Domain Adaptation을 직접 구현해 봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWlAlAeRtm4s"
      },
      "source": [
        "Office-31은 여러 실습에서 자주 등장하는 이미지 데이터셋입니다. Amazon, Webcam, DSLR 세 개의 도메인으로 구성되어 있는데요. \n",
        "\n",
        "총 31개 클래스 (키보드, 마우스, 모니터 등)의 사무용품 이미지가 있고,각 도메인마다 같은 클래스가 포함되어 있지만 도메인마다 이미지 특성은 다릅니다. \n",
        "\n",
        "그래서 이런 transfer learning 실습에 적합하다고 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from torchvision import models\n",
        "torch.cuda.set_device(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9Ut17hwUDq-"
      },
      "source": [
        "amazon을 source로, webcam을 target data로 이용해 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "j4e--IIRU68M"
      },
      "outputs": [],
      "source": [
        "data_folder = 'OFFICE31'\n",
        "batch_size = 32\n",
        "n_class = 31\n",
        "domain_src, domain_tar = 'amazon', 'webcam'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA6e2YaPtm4u"
      },
      "source": [
        "source와 target domain에 대한 DataLoader를 생성하고 load해 줍시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6JtN_bK9VFcM"
      },
      "outputs": [],
      "source": [
        "def load_data(root_path, domain, batch_size, phase):\n",
        "    transform_dict = {\n",
        "        'src': transforms.Compose(\n",
        "        [transforms.RandomResizedCrop(224),\n",
        "         transforms.RandomHorizontalFlip(),\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                              std=[0.229, 0.224, 0.225]),\n",
        "         ]),\n",
        "        'tar': transforms.Compose(\n",
        "        [transforms.Resize(224),\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                              std=[0.229, 0.224, 0.225]),\n",
        "         ])}\n",
        "    data = datasets.ImageFolder(root=os.path.join(root_path, domain), transform=transform_dict[phase])\n",
        "    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=phase=='src', drop_last=phase=='tar', num_workers=4)\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Jf_Gw2HRVJM_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source data number: 2817\n",
            "Target data number: 795\n"
          ]
        }
      ],
      "source": [
        "src_loader = load_data(data_folder, domain_src, batch_size, phase='src')\n",
        "tar_loader = load_data(data_folder, domain_tar, batch_size, phase='tar')\n",
        "print(f'Source data number: {len(src_loader.dataset)}')\n",
        "print(f'Target data number: {len(tar_loader.dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caKMn438tm4v"
      },
      "source": [
        "# Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pre-trained ResNet50을 기반으로 한 간단한 TransferModel을 정의합니다. 여기서 모델 구조는 유지하되 마지막 fc layer만 새롭게 학습되도록 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OXAjmY7pVK8t"
      },
      "outputs": [],
      "source": [
        "class TransferModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                base_model : str = 'resnet50',\n",
        "                pretrain : bool = True,\n",
        "                n_class : int = 31):\n",
        "        super(TransferModel, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.pretrain = pretrain\n",
        "        self.n_class = n_class\n",
        "        if self.base_model == 'resnet50':\n",
        "            self.model = models.resnet50(pretrained = True)\n",
        "            n_features = self.model.fc.in_features\n",
        "            fc = nn.Linear(n_features, n_class)\n",
        "            # TODO: 새로운 fc layer를 정의\n",
        "            # TODO: 모델의 fc layer를 새로운 fc로 교체\n",
        "        else:\n",
        "            # Use other models you like, such as vgg or alexnet\n",
        "            pass\n",
        "        self.model.fc.weight.data.normal_(0, 0.005)\n",
        "        self.model.fc.bias.data.fill_(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "    \n",
        "    def predict(self, x):\n",
        "        return self.forward(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAqT-0jRtm4w"
      },
      "source": [
        "모델이 정상적으로 작동하는지 random tensor로 테스트해 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LewRmYIvXEIo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/homework/homeowkr/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/root/homework/homeowkr/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:08<00:00, 11.9MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 1.0608e-01,  1.5686e-01, -1.7205e-01, -2.6448e-02,  1.0436e-01,\n",
            "          8.3846e-02,  2.3460e-01,  3.1370e-02,  1.0277e-01,  2.5492e-01,\n",
            "          1.7499e-01,  2.9811e-01,  3.1684e-02, -1.1507e-01,  6.8100e-02,\n",
            "          8.9936e-02,  1.0294e-01,  6.8319e-02,  1.0692e-01, -1.9566e-02,\n",
            "          3.6777e-02,  6.6151e-02, -5.2812e-02,  1.5941e-03, -7.8918e-02,\n",
            "          9.3465e-02,  1.4114e-01, -6.6398e-02,  2.0725e-01,  1.5934e-01,\n",
            "          2.2541e-02, -7.7266e-02, -7.7458e-02,  3.0367e-02,  1.7767e-01,\n",
            "          9.6998e-02,  1.3635e-01,  1.6458e-01,  1.3113e-01,  3.4898e-01,\n",
            "         -1.8907e-01,  2.9936e-01,  2.3542e-02,  1.6892e-01,  8.3715e-02,\n",
            "          9.1230e-02, -1.6265e-02,  1.0658e-01,  1.1455e-01,  1.8490e-01,\n",
            "          7.4383e-02,  1.5326e-01,  6.6483e-02,  1.7782e-01,  1.8577e-01,\n",
            "          9.3435e-02,  2.1093e-01,  3.1351e-03,  2.4789e-01,  1.2559e-01,\n",
            "          6.2745e-02,  1.2172e-01,  1.4194e-02,  5.1865e-02, -2.3006e-02,\n",
            "         -8.4161e-02, -1.7575e-03,  2.4928e-01,  1.1741e-02,  6.4153e-02,\n",
            "          1.9191e-01,  3.3237e-02,  7.2933e-02, -5.0939e-02,  2.5872e-01,\n",
            "          2.0471e-01,  4.5552e-02,  9.0547e-02,  1.3330e-01,  1.0916e-01,\n",
            "          1.4581e-01,  1.3492e-01,  1.1661e-01,  1.1717e-01,  1.2821e-01,\n",
            "         -1.6330e-01,  7.9443e-02, -4.3817e-02,  1.3957e-01,  1.0024e-01,\n",
            "         -3.6143e-02, -6.9542e-02,  2.3807e-01, -4.9450e-02,  5.7707e-02,\n",
            "          8.8242e-02,  1.8793e-01,  1.0478e-01,  3.2365e-01, -5.5008e-02,\n",
            "          3.0750e-01, -2.3536e-02,  1.0580e-01,  3.1751e-02,  2.5308e-01,\n",
            "          2.1549e-01,  1.0820e-01,  1.9626e-01,  1.5759e-01,  4.7138e-01,\n",
            "          1.1650e-01,  3.0966e-02,  1.0079e-01,  1.0201e-01, -2.8586e-02,\n",
            "          9.4598e-02,  1.5834e-01,  2.7427e-01, -1.3453e-02,  2.3139e-01,\n",
            "         -1.1714e-01,  1.1756e-01,  8.6611e-02,  1.4087e-01,  2.0562e-01,\n",
            "          8.9268e-02,  1.0582e-01,  5.1276e-02,  5.9548e-02,  1.2556e-01,\n",
            "          8.7622e-02, -2.5541e-02,  1.4643e-01,  2.6594e-01,  1.4203e-01,\n",
            "          1.9296e-01,  1.7424e-01,  4.1725e-02,  1.5065e-01,  1.2382e-01,\n",
            "          2.2737e-01,  2.8301e-01, -8.4779e-03,  1.5316e-01,  1.4217e-01,\n",
            "          2.4723e-01,  1.4634e-01,  4.0964e-02,  2.8535e-01,  2.8514e-01,\n",
            "          1.6943e-01,  1.2394e-01,  6.5980e-02,  1.7501e-01,  3.6555e-02,\n",
            "         -5.6785e-03,  3.3348e-01,  9.1661e-02,  1.5238e-01,  3.5900e-01,\n",
            "          1.4131e-02,  1.0069e-01,  1.6804e-01,  8.7164e-02,  1.1977e-01,\n",
            "          1.3762e-01,  1.2996e-01, -1.6602e-01,  2.7902e-01,  3.8963e-02,\n",
            "          7.4743e-02,  3.7649e-01,  1.5677e-01,  9.4404e-02,  7.6392e-02,\n",
            "          2.5366e-01,  4.2666e-03, -1.1609e-01,  1.7431e-01,  4.6654e-02,\n",
            "         -4.2649e-02,  1.6278e-01,  5.2288e-02,  1.3978e-02,  1.5699e-01,\n",
            "          2.3740e-02,  9.4003e-02,  2.1084e-01,  9.0737e-02,  1.0231e-02,\n",
            "          3.0956e-01, -5.8500e-02,  9.0526e-02,  1.8024e-01, -1.6798e-01,\n",
            "          5.1347e-02,  1.6428e-01,  2.1914e-01, -1.5950e-02,  1.6280e-01,\n",
            "          1.3557e-01,  3.1949e-01,  4.8921e-02,  7.6664e-02,  3.3013e-02,\n",
            "          1.4228e-01,  1.6794e-01,  7.7810e-02,  1.9024e-01,  5.4301e-02,\n",
            "          2.2333e-01, -2.5739e-03,  1.8133e-01,  1.4429e-01,  3.8951e-02,\n",
            "          5.0841e-02, -7.8689e-02,  2.4372e-01,  2.8141e-01, -1.0841e-02,\n",
            "          1.8327e-01,  1.2783e-01,  2.6043e-01,  1.5893e-01,  9.8702e-02,\n",
            "          1.6201e-01,  2.4376e-02,  2.6364e-01,  1.6237e-01,  3.9313e-02,\n",
            "          1.3916e-01,  2.3972e-01,  5.8915e-02,  5.9428e-02,  2.0470e-01,\n",
            "         -6.2148e-02, -3.7248e-02,  2.9658e-01,  8.8140e-02,  4.3038e-02,\n",
            "          2.6595e-01,  1.2820e-01,  1.7768e-01,  1.6047e-01,  2.2290e-01,\n",
            "          1.3817e-01,  8.0810e-02,  8.6028e-02,  5.2709e-02,  1.1457e-01,\n",
            "          4.6051e-02,  1.2033e-01,  1.6320e-01,  3.9254e-02,  2.2784e-01,\n",
            "          1.2115e-01, -5.9339e-02,  2.0975e-01,  4.8280e-03,  1.4307e-01,\n",
            "          2.5774e-01,  1.8524e-01,  1.1504e-01, -1.5208e-01,  2.6419e-02,\n",
            "          1.8962e-01,  6.3657e-02,  1.6774e-01,  2.3762e-01,  2.2618e-01,\n",
            "          8.4542e-02,  2.5974e-01,  1.2295e-01,  1.4295e-01,  6.7721e-02,\n",
            "          8.7681e-02,  6.5178e-02,  2.4272e-01,  1.6442e-01,  1.6576e-01,\n",
            "          9.8748e-02,  4.7610e-02,  3.3104e-01,  2.9052e-01, -5.9732e-02,\n",
            "         -1.5339e-01,  1.6518e-01,  1.5210e-01,  4.6719e-02,  2.4170e-01,\n",
            "          1.2357e-01,  2.4055e-01, -3.4428e-02,  2.9725e-01,  2.6105e-01,\n",
            "          1.0837e-01,  2.2552e-01,  7.6293e-02,  6.5334e-03,  8.0181e-02,\n",
            "         -9.9868e-02,  1.6926e-01,  1.8065e-01,  2.4522e-01,  7.0108e-02,\n",
            "          2.8874e-01,  8.9544e-02,  7.3928e-02,  1.7926e-01,  9.7915e-02,\n",
            "          6.4005e-02,  1.1101e-01,  1.8542e-01, -3.4434e-02,  1.4825e-01,\n",
            "          5.6488e-02,  6.1398e-02,  1.6233e-01,  9.2662e-02, -4.8196e-02,\n",
            "          1.0210e-01,  9.9708e-02,  3.4786e-02,  1.7709e-01,  2.0128e-01,\n",
            "          6.4927e-02,  2.9748e-03,  8.3345e-02, -3.9006e-03,  1.5087e-01,\n",
            "          9.6456e-02,  2.7210e-01, -3.4186e-01, -1.1458e-01, -1.3719e-01,\n",
            "          4.4722e-02,  1.6493e-01, -9.3905e-03,  4.2138e-02,  1.1122e-01,\n",
            "          4.4729e-02,  1.8726e-01,  1.1168e-01,  7.0512e-02,  1.3275e-01,\n",
            "         -4.0124e-02,  1.2687e-01,  2.4288e-02,  2.4971e-02,  7.5798e-02,\n",
            "         -6.9124e-02,  1.3402e-02,  5.6666e-02,  1.2947e-01,  1.3760e-01,\n",
            "          1.7223e-01, -5.0349e-02,  1.4280e-01,  2.5425e-01, -1.2378e-01,\n",
            "          1.5156e-01,  3.6967e-01,  1.0932e-01,  5.3266e-02,  2.3328e-01,\n",
            "          1.0772e-01,  1.1414e-01,  1.3188e-01, -3.8295e-02,  1.2861e-01,\n",
            "          1.1419e-01,  2.3582e-01,  1.4193e-01,  5.0911e-03,  6.7689e-02,\n",
            "          1.7604e-01,  1.5316e-01,  9.0588e-02,  5.5152e-02,  3.8741e-02,\n",
            "         -1.9255e-02,  9.1931e-02,  1.1416e-01,  1.9552e-01,  1.0239e-01,\n",
            "          1.8711e-01,  2.6099e-01, -1.6461e-01,  1.1891e-01,  1.2486e-01,\n",
            "          1.7530e-01,  1.0780e-01, -1.4092e-02,  1.4105e-01, -1.4748e-02,\n",
            "          1.1037e-01,  5.2901e-02,  2.7049e-01,  1.4239e-01, -8.8826e-03,\n",
            "          1.6046e-01, -7.8436e-02,  1.6129e-01,  1.8896e-01,  3.5965e-02,\n",
            "          1.5366e-02,  5.9290e-02,  1.8981e-01,  1.5206e-01,  9.6075e-02,\n",
            "         -7.1606e-02,  9.9706e-02,  8.0214e-02,  1.4629e-01, -2.0170e-02,\n",
            "         -5.5429e-02, -1.8782e-02,  6.5588e-02, -4.0722e-03,  6.0862e-02,\n",
            "          5.5835e-02,  4.2676e-01,  9.0097e-02,  1.7316e-01,  6.4289e-02,\n",
            "          1.9871e-01,  5.0686e-02,  2.5337e-01,  4.5537e-02,  3.2498e-01,\n",
            "          1.3630e-02,  6.2912e-02,  4.8611e-02,  1.2390e-01,  8.5815e-02,\n",
            "          3.6364e-02,  9.8945e-02,  1.7040e-02,  1.8449e-01,  2.5294e-01,\n",
            "         -1.0819e-01,  1.3588e-01,  1.4319e-01,  1.4656e-01,  7.2783e-02,\n",
            "         -1.5021e-03,  2.9720e-01,  8.1997e-03,  2.0889e-01,  6.9687e-02,\n",
            "          1.1494e-01, -5.0538e-02,  9.2857e-02,  8.1003e-02,  1.6674e-01,\n",
            "          5.7208e-02,  1.2938e-01,  1.5396e-03,  1.0810e-01,  1.6770e-01,\n",
            "          6.4172e-02,  9.8445e-02, -3.4386e-03,  5.4488e-02,  1.3435e-01,\n",
            "          2.1254e-01,  2.9408e-02,  4.3819e-02,  5.0885e-03,  2.8323e-01,\n",
            "          1.9157e-01,  1.1971e-01,  2.0002e-01,  4.3480e-02,  8.1550e-02,\n",
            "          1.5485e-01, -1.0260e-02,  2.5046e-01,  2.0321e-02,  4.8181e-02,\n",
            "          1.0964e-01,  1.6666e-01,  4.8118e-02,  7.0868e-02,  1.7425e-01,\n",
            "         -1.7062e-02,  2.0953e-01,  2.8658e-01,  2.0449e-01,  2.3752e-01,\n",
            "          1.1614e-01,  9.7269e-02,  5.5466e-02,  2.2939e-02,  3.5229e-01,\n",
            "          1.5783e-01,  1.8165e-01, -3.9257e-02,  1.7570e-01,  3.3819e-01,\n",
            "          2.9112e-02, -6.4301e-03,  2.0356e-01,  6.4577e-02, -9.0381e-02,\n",
            "          1.1410e-01,  9.3054e-02,  6.5021e-02,  1.9022e-01,  2.6347e-01,\n",
            "          1.3385e-01,  2.1880e-01,  1.0884e-01,  3.9573e-02,  7.3503e-02,\n",
            "          2.2319e-01, -3.5066e-02,  1.7545e-01, -1.3981e-01,  5.9580e-02,\n",
            "          1.4648e-02,  1.9376e-02,  4.6867e-02,  1.4551e-01, -7.7763e-02,\n",
            "         -7.3220e-02,  1.1036e-01, -1.0308e-01,  1.8419e-01,  1.5647e-01,\n",
            "          2.9189e-01, -6.5787e-02,  3.3591e-03,  1.2336e-01,  4.3662e-02,\n",
            "          1.7599e-01,  1.9807e-01,  1.8256e-01,  9.1842e-02, -7.4552e-02,\n",
            "          1.5832e-01,  1.9466e-01,  1.2116e-02,  1.7939e-01,  2.3243e-01,\n",
            "          2.2115e-01,  2.0638e-01,  1.0407e-01,  7.4094e-02, -1.1404e-02,\n",
            "          2.8667e-02,  8.0666e-02,  4.6450e-02, -1.1515e-02,  8.1815e-02,\n",
            "         -7.3798e-02,  1.2805e-01,  6.0138e-02,  2.2963e-02,  1.0530e-02,\n",
            "          8.2596e-02,  8.5729e-02,  1.1662e-01,  1.8680e-01,  1.1008e-01,\n",
            "         -5.6790e-03,  1.8778e-01,  1.5217e-01,  2.1421e-01,  1.3008e-01,\n",
            "          2.5535e-01,  1.9629e-01,  1.1790e-01,  1.5982e-01,  7.8234e-02,\n",
            "          1.3050e-01,  3.1299e-01,  1.4236e-01,  1.5798e-01,  2.3069e-01,\n",
            "          1.9897e-01,  5.6055e-02,  3.2026e-02, -3.5541e-02,  1.3257e-01,\n",
            "          5.6486e-02,  3.1943e-02,  5.2037e-02,  2.9284e-01,  7.3466e-02,\n",
            "          1.2867e-01,  2.6630e-02, -1.9069e-02,  2.4142e-01,  9.9477e-02,\n",
            "         -4.4024e-02,  8.1584e-02,  9.7925e-02,  9.2988e-02,  7.6837e-02,\n",
            "          9.9032e-02,  3.2030e-02,  2.3667e-01, -1.1363e-02,  1.1332e-01,\n",
            "          1.4476e-01, -9.8527e-02,  1.3056e-01,  8.1815e-02, -1.4657e-02,\n",
            "         -7.3053e-02,  7.1038e-02, -3.3306e-02,  3.8585e-02,  1.0042e-01,\n",
            "          2.4198e-02,  3.6718e-02,  4.7889e-02,  1.0243e-01,  1.3337e-01,\n",
            "          1.2835e-01,  7.1094e-02, -1.2821e-01,  1.5217e-01, -3.5920e-02,\n",
            "         -2.3350e-02,  9.4594e-02,  9.8318e-02,  1.9857e-01,  2.0997e-01,\n",
            "          7.0234e-02,  6.1752e-02,  1.7961e-01,  1.3159e-01,  2.2584e-01,\n",
            "          1.3580e-01,  1.4845e-01,  1.0664e-01,  8.2077e-02,  8.2734e-02,\n",
            "          8.9466e-02,  3.0842e-01,  1.9155e-01, -1.1212e-01,  5.2802e-02,\n",
            "         -5.9132e-02,  1.2484e-02,  1.6730e-01,  8.3729e-02, -7.8688e-02,\n",
            "          9.3106e-02,  6.7245e-02, -1.4841e-01, -3.1594e-02,  1.6516e-01,\n",
            "          7.5324e-02,  1.3052e-01, -4.5219e-04,  4.3487e-02,  2.1879e-01,\n",
            "         -1.8383e-01,  3.0530e-02,  1.8776e-01,  2.4639e-01,  1.8029e-01,\n",
            "         -8.7815e-03,  2.2065e-02, -1.4704e-02, -3.3624e-02,  6.7066e-02,\n",
            "          6.8604e-02,  1.0277e-01, -8.9076e-03, -2.0544e-02, -2.2803e-03,\n",
            "          1.3650e-01,  5.4593e-02,  1.5711e-02,  1.8823e-01,  1.2530e-01,\n",
            "          1.4346e-01, -2.6605e-02,  1.5155e-01, -6.2726e-02,  8.5174e-02,\n",
            "          1.2483e-01,  5.0744e-02,  5.4676e-03,  4.5953e-02,  2.6551e-01,\n",
            "          1.2716e-02,  2.0219e-01,  1.6117e-01,  1.3644e-01,  1.8278e-01,\n",
            "          1.6623e-01, -1.2156e-02,  2.5416e-01, -7.4174e-03, -7.2931e-02,\n",
            "          2.3776e-01,  6.7107e-02,  1.4909e-01,  1.8015e-02,  4.6472e-02,\n",
            "          2.1609e-02,  6.4601e-02,  1.8197e-01,  8.6262e-02,  2.0421e-01,\n",
            "          9.9363e-02, -3.8468e-02,  1.4432e-01,  2.4182e-01,  1.2384e-01,\n",
            "          3.4520e-01,  9.1468e-02,  2.3606e-01,  1.3666e-01,  1.6332e-01,\n",
            "         -4.7340e-02, -4.5713e-02, -1.4004e-01,  6.5365e-02, -1.1068e-01,\n",
            "          9.2879e-02,  3.2914e-01,  1.2219e-01,  1.0140e-01, -2.4821e-02,\n",
            "          8.8528e-02,  1.7799e-02,  2.1431e-01, -4.3222e-02,  4.6965e-01,\n",
            "          3.5638e-02,  1.5073e-02,  1.3823e-01, -1.3057e-02,  9.6254e-02,\n",
            "          1.9704e-01,  7.9581e-02,  1.0820e-01, -6.6892e-02,  5.6297e-02,\n",
            "          4.3270e-02,  1.2883e-02,  1.2915e-01,  2.6924e-02, -1.3476e-01,\n",
            "          1.0242e-01,  1.7345e-01, -9.6177e-03, -1.5784e-01,  8.2529e-02,\n",
            "          3.2660e-01,  1.8730e-01,  9.2132e-02,  2.0305e-01,  1.8442e-01,\n",
            "          1.0613e-01,  1.8644e-01,  6.2134e-02,  1.3328e-01,  4.1448e-02,\n",
            "          1.7504e-01,  5.8938e-02,  8.5665e-02,  8.1617e-02,  1.0934e-01,\n",
            "          4.7282e-02,  1.0716e-01,  1.3712e-01,  1.5815e-01,  7.2129e-02,\n",
            "          1.5191e-01,  7.2896e-02, -1.6409e-02,  2.2632e-01,  9.1113e-02,\n",
            "          1.6401e-01,  1.0277e-01,  9.6163e-02,  1.1530e-01, -5.0156e-03,\n",
            "         -6.0719e-02,  1.2456e-01, -6.6713e-02,  1.6885e-01,  1.4483e-01,\n",
            "          6.5645e-02, -2.2364e-02,  1.0354e-01, -7.6598e-02,  3.9041e-02,\n",
            "          1.1294e-01,  2.0769e-01,  6.7315e-02,  1.0389e-01,  2.1397e-01,\n",
            "         -8.7507e-02,  3.2102e-01,  1.3387e-01,  5.0112e-02, -1.4570e-01,\n",
            "          2.3409e-02, -1.5462e-02,  8.2817e-02,  2.9368e-02,  3.3782e-01,\n",
            "          1.6606e-01,  6.2543e-03,  2.1498e-01,  1.7300e-01,  2.3193e-01,\n",
            "          1.0401e-01,  2.7489e-01,  1.2909e-02,  1.2139e-01,  4.4591e-03,\n",
            "          2.4327e-01,  1.2021e-01,  1.1381e-01,  2.1611e-01,  7.5827e-02,\n",
            "          8.9947e-02,  1.1812e-01,  2.1440e-01,  1.8564e-02,  4.0038e-02,\n",
            "          1.3549e-01,  3.2356e-01, -3.7371e-02, -7.3603e-03,  1.1715e-02,\n",
            "          1.9813e-01,  5.5151e-02,  3.3330e-01,  7.7105e-02,  2.7887e-01,\n",
            "          1.5533e-01,  1.0863e-01, -4.5814e-02,  1.8388e-02,  1.4919e-01,\n",
            "          8.0306e-02,  9.8109e-02,  3.0735e-01,  1.8377e-01,  6.1345e-02,\n",
            "         -5.5553e-02,  3.2682e-02,  5.5099e-02,  1.0197e-01,  2.2481e-01,\n",
            "         -2.2509e-01,  7.4907e-02, -1.4169e-01,  2.1642e-02,  5.3476e-02,\n",
            "          1.5804e-01,  2.5895e-01,  1.5485e-01,  2.3390e-01,  1.6592e-01,\n",
            "          1.1863e-01,  3.0299e-02,  1.1220e-01,  1.9005e-01,  3.9261e-01,\n",
            "         -8.4877e-02, -1.7636e-02,  1.0298e-01,  1.9246e-01,  5.5168e-02,\n",
            "          1.3586e-01,  1.3621e-01, -7.0876e-02,  1.6001e-01,  9.4258e-02,\n",
            "          8.7003e-02, -2.9527e-02,  1.1712e-01,  1.3545e-01,  2.7825e-01,\n",
            "          2.1009e-01,  1.0402e-01,  5.6381e-03,  1.2198e-01,  2.2420e-01,\n",
            "          2.4971e-01,  5.8812e-02, -9.1612e-02,  1.9147e-01,  1.4669e-01,\n",
            "          3.5793e-02, -4.9096e-03,  7.7607e-03, -1.3091e-02, -2.7313e-02,\n",
            "          2.3914e-01,  4.9448e-02,  1.8840e-01,  9.4142e-02,  3.2030e-01,\n",
            "          7.9953e-02,  1.0480e-01,  1.9195e-01, -1.4038e-02,  1.3984e-01,\n",
            "          2.2865e-02,  8.1593e-02,  2.6463e-01,  1.4409e-01,  1.6487e-01,\n",
            "         -7.8440e-03, -8.1309e-02, -3.4422e-02, -7.8644e-02,  8.0460e-02,\n",
            "          7.9237e-02,  2.1955e-01, -5.3430e-02,  1.4821e-01,  1.4616e-01,\n",
            "          4.9672e-02,  1.7124e-01,  1.0083e-01,  9.8997e-02,  1.0996e-01,\n",
            "          5.4099e-02,  1.2911e-01,  4.1831e-02, -3.0796e-02,  2.1877e-01,\n",
            "          1.9059e-01,  1.5618e-01,  2.4925e-01,  6.9832e-02, -1.6344e-01,\n",
            "          5.7451e-02,  2.2564e-01,  2.4887e-01,  1.8789e-01,  7.3598e-02,\n",
            "          2.8541e-01, -6.7034e-02,  1.1662e-01,  1.7883e-01, -1.2743e-01,\n",
            "          2.3100e-01,  3.5299e-02, -9.3834e-02,  2.3123e-01,  1.6441e-01,\n",
            "          3.2118e-02,  1.5578e-01,  1.7684e-02,  2.6241e-01, -2.6681e-02,\n",
            "          4.6457e-02,  1.5387e-01,  1.0242e-01,  1.0478e-01,  1.9766e-01,\n",
            "          1.6853e-01,  2.0063e-02,  9.8015e-02, -7.0447e-02, -9.9449e-02,\n",
            "          1.1553e-01,  2.0961e-01,  1.4416e-01,  6.3749e-02,  1.0209e-01,\n",
            "          3.4462e-01,  1.2030e-01, -7.3922e-02,  6.8584e-02, -4.7246e-02,\n",
            "         -6.2007e-02,  2.8821e-01,  3.9036e-02,  1.9392e-01,  5.4883e-02,\n",
            "          1.1756e-01, -7.2241e-02,  1.9208e-01,  1.9957e-01,  1.2197e-01,\n",
            "          1.2752e-01,  6.0528e-02, -3.7191e-02, -3.2012e-02,  7.3446e-02,\n",
            "         -4.9170e-02,  1.5870e-01,  1.7800e-01, -5.7689e-02,  2.9742e-02]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "torch.Size([1, 1000])\n"
          ]
        }
      ],
      "source": [
        "model = TransferModel().cuda()\n",
        "RAND_TENSOR = torch.randn(1, 3, 224, 224).cuda()\n",
        "output = model(RAND_TENSOR)\n",
        "print(output)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpX-fuiXtm4w"
      },
      "source": [
        "## Finetune ResNet-50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uNeAiPatm4w"
      },
      "source": [
        "Office-31 dataset은 validation set이 따로 없으므로, validation set으로 target domain을 이용해 줍시다.\n",
        "\n",
        "fine-tuning을 위한 학습 및 평가 함수를 정의합니다.\n",
        "학습은 source domain에서 수행하고 target domain에서 검증합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "h74gKIVqtm4w"
      },
      "outputs": [],
      "source": [
        "dataloaders = {'src': src_loader,\n",
        "               'val': tar_loader,\n",
        "               'tar': tar_loader}\n",
        "n_epoch = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "early_stop = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-fz_FlAIXsTF"
      },
      "outputs": [],
      "source": [
        "def finetune(model, dataloaders, optimizer):\n",
        "    since = time.time()\n",
        "    best_acc = 0\n",
        "    stop = 0\n",
        "    for epoch in range(0, n_epoch):\n",
        "        stop += 1\n",
        "        # You can uncomment this line for scheduling learning rate\n",
        "        # lr_schedule(optimizer, epoch)\n",
        "        for phase in ['src', 'val', 'tar']:\n",
        "            if phase == 'src':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            total_loss, correct = 0, 0\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "                optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(phase == 'src'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                preds = torch.max(outputs, 1)[1]\n",
        "                if phase == 'src':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                total_loss += loss.item() * inputs.size(0)\n",
        "                correct += torch.sum(preds == labels.data)\n",
        "            epoch_loss = total_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = correct.double() / len(dataloaders[phase].dataset)\n",
        "            print(f'Epoch: [{epoch:02d}/{n_epoch:02d}]---{phase}, loss: {epoch_loss:.6f}, acc: {epoch_acc:.4f}')\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                stop = 0\n",
        "                best_acc = epoch_acc\n",
        "                torch.save(model.state_dict(), 'model.pkl')\n",
        "        if stop >= early_stop:\n",
        "            break\n",
        "        print()\n",
        "   \n",
        "    time_pass = time.time() - since\n",
        "    print(f'Training complete in {time_pass // 60:.0f}m {time_pass % 60:.0f}s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGUZGrm7tm4x"
      },
      "source": [
        "이제 학습 파라미터들과 optimizer를 정의합니다.\n",
        "간단하게 SGD optimizer를 사용하고 fc layer의 학습률을 다른 layer보다 10배 크게 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HGVIu0JXZaGT"
      },
      "outputs": [],
      "source": [
        "param_group = []\n",
        "learning_rate = 0.0001\n",
        "momentum = 5e-4\n",
        "for k, v in model.named_parameters():\n",
        "    if not k.__contains__('fc'):\n",
        "        param_group += [{'params': v, 'lr': learning_rate}]\n",
        "    else:\n",
        "        param_group += [{'params': v, 'lr': learning_rate * 10}]\n",
        "optimizer = torch.optim.SGD(param_group, momentum=momentum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7zGR_PFtm4y"
      },
      "source": [
        "## Train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uKKrah-AZsHt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [00/100]---src, loss: 6.311683, acc: 0.0966\n",
            "Epoch: [00/100]---val, loss: 5.350757, acc: 0.0579\n",
            "Epoch: [00/100]---tar, loss: 5.350757, acc: 0.0579\n",
            "\n",
            "Epoch: [01/100]---src, loss: 5.131718, acc: 0.1505\n",
            "Epoch: [01/100]---val, loss: 4.207622, acc: 0.1925\n",
            "Epoch: [01/100]---tar, loss: 4.207622, acc: 0.1925\n",
            "\n",
            "Epoch: [02/100]---src, loss: 4.273892, acc: 0.3507\n",
            "Epoch: [02/100]---val, loss: 3.590667, acc: 0.3597\n",
            "Epoch: [02/100]---tar, loss: 3.590667, acc: 0.3597\n",
            "\n",
            "Epoch: [03/100]---src, loss: 3.721603, acc: 0.4764\n",
            "Epoch: [03/100]---val, loss: 3.242523, acc: 0.4340\n",
            "Epoch: [03/100]---tar, loss: 3.242523, acc: 0.4340\n",
            "\n",
            "Epoch: [04/100]---src, loss: 3.368942, acc: 0.5563\n",
            "Epoch: [04/100]---val, loss: 2.974478, acc: 0.4943\n",
            "Epoch: [04/100]---tar, loss: 2.974478, acc: 0.4943\n",
            "\n",
            "Epoch: [05/100]---src, loss: 3.113646, acc: 0.5584\n",
            "Epoch: [05/100]---val, loss: 2.797569, acc: 0.5648\n",
            "Epoch: [05/100]---tar, loss: 2.797569, acc: 0.5648\n",
            "\n",
            "Epoch: [06/100]---src, loss: 2.912075, acc: 0.6099\n",
            "Epoch: [06/100]---val, loss: 2.696896, acc: 0.5937\n",
            "Epoch: [06/100]---tar, loss: 2.696896, acc: 0.5937\n",
            "\n",
            "Epoch: [07/100]---src, loss: 2.745664, acc: 0.6248\n",
            "Epoch: [07/100]---val, loss: 2.513368, acc: 0.6289\n",
            "Epoch: [07/100]---tar, loss: 2.513368, acc: 0.6289\n",
            "\n",
            "Epoch: [08/100]---src, loss: 2.580006, acc: 0.6322\n",
            "Epoch: [08/100]---val, loss: 2.431972, acc: 0.6365\n",
            "Epoch: [08/100]---tar, loss: 2.431972, acc: 0.6365\n",
            "\n",
            "Epoch: [09/100]---src, loss: 2.443777, acc: 0.6628\n",
            "Epoch: [09/100]---val, loss: 2.306359, acc: 0.6264\n",
            "Epoch: [09/100]---tar, loss: 2.306359, acc: 0.6264\n",
            "\n",
            "Epoch: [10/100]---src, loss: 2.329442, acc: 0.6613\n",
            "Epoch: [10/100]---val, loss: 2.198305, acc: 0.6503\n",
            "Epoch: [10/100]---tar, loss: 2.198305, acc: 0.6503\n",
            "\n",
            "Epoch: [11/100]---src, loss: 2.221175, acc: 0.6713\n",
            "Epoch: [11/100]---val, loss: 2.093244, acc: 0.6604\n",
            "Epoch: [11/100]---tar, loss: 2.093244, acc: 0.6604\n",
            "\n",
            "Epoch: [12/100]---src, loss: 2.094363, acc: 0.6830\n",
            "Epoch: [12/100]---val, loss: 2.019384, acc: 0.6616\n",
            "Epoch: [12/100]---tar, loss: 2.019384, acc: 0.6616\n",
            "\n",
            "Epoch: [13/100]---src, loss: 2.022992, acc: 0.6791\n",
            "Epoch: [13/100]---val, loss: 1.913596, acc: 0.6616\n",
            "Epoch: [13/100]---tar, loss: 1.913596, acc: 0.6616\n",
            "\n",
            "Epoch: [14/100]---src, loss: 1.936850, acc: 0.6844\n",
            "Epoch: [14/100]---val, loss: 1.857818, acc: 0.6667\n",
            "Epoch: [14/100]---tar, loss: 1.857818, acc: 0.6667\n",
            "\n",
            "Epoch: [15/100]---src, loss: 1.850626, acc: 0.7004\n",
            "Epoch: [15/100]---val, loss: 1.802006, acc: 0.6956\n",
            "Epoch: [15/100]---tar, loss: 1.802006, acc: 0.6956\n",
            "\n",
            "Epoch: [16/100]---src, loss: 1.774162, acc: 0.7082\n",
            "Epoch: [16/100]---val, loss: 1.716844, acc: 0.6730\n",
            "Epoch: [16/100]---tar, loss: 1.716844, acc: 0.6730\n",
            "\n",
            "Epoch: [17/100]---src, loss: 1.722489, acc: 0.7029\n",
            "Epoch: [17/100]---val, loss: 1.706201, acc: 0.6767\n",
            "Epoch: [17/100]---tar, loss: 1.706201, acc: 0.6767\n",
            "\n",
            "Epoch: [18/100]---src, loss: 1.671512, acc: 0.7050\n",
            "Epoch: [18/100]---val, loss: 1.616092, acc: 0.6969\n",
            "Epoch: [18/100]---tar, loss: 1.616092, acc: 0.6969\n",
            "\n",
            "Epoch: [19/100]---src, loss: 1.612874, acc: 0.7196\n",
            "Epoch: [19/100]---val, loss: 1.596987, acc: 0.6981\n",
            "Epoch: [19/100]---tar, loss: 1.596987, acc: 0.6981\n",
            "\n",
            "Epoch: [20/100]---src, loss: 1.545802, acc: 0.7228\n",
            "Epoch: [20/100]---val, loss: 1.536950, acc: 0.7006\n",
            "Epoch: [20/100]---tar, loss: 1.536950, acc: 0.7006\n",
            "\n",
            "Epoch: [21/100]---src, loss: 1.496200, acc: 0.7235\n",
            "Epoch: [21/100]---val, loss: 1.489391, acc: 0.6868\n",
            "Epoch: [21/100]---tar, loss: 1.489391, acc: 0.6868\n",
            "\n",
            "Epoch: [22/100]---src, loss: 1.472868, acc: 0.7238\n",
            "Epoch: [22/100]---val, loss: 1.460132, acc: 0.6868\n",
            "Epoch: [22/100]---tar, loss: 1.460132, acc: 0.6868\n",
            "\n",
            "Epoch: [23/100]---src, loss: 1.441781, acc: 0.7174\n",
            "Epoch: [23/100]---val, loss: 1.458553, acc: 0.6843\n",
            "Epoch: [23/100]---tar, loss: 1.458553, acc: 0.6843\n",
            "\n",
            "Epoch: [24/100]---src, loss: 1.404782, acc: 0.7330\n",
            "Epoch: [24/100]---val, loss: 1.389150, acc: 0.6994\n",
            "Epoch: [24/100]---tar, loss: 1.389150, acc: 0.6994\n",
            "\n",
            "Epoch: [25/100]---src, loss: 1.341632, acc: 0.7355\n",
            "Epoch: [25/100]---val, loss: 1.397107, acc: 0.7031\n",
            "Epoch: [25/100]---tar, loss: 1.397107, acc: 0.7031\n",
            "\n",
            "Epoch: [26/100]---src, loss: 1.314887, acc: 0.7469\n",
            "Epoch: [26/100]---val, loss: 1.328589, acc: 0.7094\n",
            "Epoch: [26/100]---tar, loss: 1.328589, acc: 0.7094\n",
            "\n",
            "Epoch: [27/100]---src, loss: 1.287913, acc: 0.7398\n",
            "Epoch: [27/100]---val, loss: 1.302140, acc: 0.7195\n",
            "Epoch: [27/100]---tar, loss: 1.302140, acc: 0.7195\n",
            "\n",
            "Epoch: [28/100]---src, loss: 1.251302, acc: 0.7533\n",
            "Epoch: [28/100]---val, loss: 1.312580, acc: 0.7031\n",
            "Epoch: [28/100]---tar, loss: 1.312580, acc: 0.7031\n",
            "\n",
            "Epoch: [29/100]---src, loss: 1.247743, acc: 0.7476\n",
            "Epoch: [29/100]---val, loss: 1.266595, acc: 0.7132\n",
            "Epoch: [29/100]---tar, loss: 1.266595, acc: 0.7132\n",
            "\n",
            "Epoch: [30/100]---src, loss: 1.231225, acc: 0.7455\n",
            "Epoch: [30/100]---val, loss: 1.303708, acc: 0.6969\n",
            "Epoch: [30/100]---tar, loss: 1.303708, acc: 0.6969\n",
            "\n",
            "Epoch: [31/100]---src, loss: 1.213395, acc: 0.7444\n",
            "Epoch: [31/100]---val, loss: 1.224786, acc: 0.7333\n",
            "Epoch: [31/100]---tar, loss: 1.224786, acc: 0.7333\n",
            "\n",
            "Epoch: [32/100]---src, loss: 1.173787, acc: 0.7551\n",
            "Epoch: [32/100]---val, loss: 1.195142, acc: 0.7245\n",
            "Epoch: [32/100]---tar, loss: 1.195142, acc: 0.7245\n",
            "\n",
            "Epoch: [33/100]---src, loss: 1.171331, acc: 0.7430\n",
            "Epoch: [33/100]---val, loss: 1.162115, acc: 0.7233\n",
            "Epoch: [33/100]---tar, loss: 1.162115, acc: 0.7233\n",
            "\n",
            "Epoch: [34/100]---src, loss: 1.150381, acc: 0.7504\n",
            "Epoch: [34/100]---val, loss: 1.153556, acc: 0.7132\n",
            "Epoch: [34/100]---tar, loss: 1.153556, acc: 0.7132\n",
            "\n",
            "Epoch: [35/100]---src, loss: 1.125100, acc: 0.7543\n",
            "Epoch: [35/100]---val, loss: 1.131223, acc: 0.7283\n",
            "Epoch: [35/100]---tar, loss: 1.131223, acc: 0.7283\n",
            "\n",
            "Epoch: [36/100]---src, loss: 1.097136, acc: 0.7583\n",
            "Epoch: [36/100]---val, loss: 1.135353, acc: 0.7170\n",
            "Epoch: [36/100]---tar, loss: 1.135353, acc: 0.7170\n",
            "\n",
            "Epoch: [37/100]---src, loss: 1.093543, acc: 0.7575\n",
            "Epoch: [37/100]---val, loss: 1.113804, acc: 0.7384\n",
            "Epoch: [37/100]---tar, loss: 1.113804, acc: 0.7384\n",
            "\n",
            "Epoch: [38/100]---src, loss: 1.075184, acc: 0.7590\n",
            "Epoch: [38/100]---val, loss: 1.085096, acc: 0.7384\n",
            "Epoch: [38/100]---tar, loss: 1.085096, acc: 0.7384\n",
            "\n",
            "Epoch: [39/100]---src, loss: 1.075434, acc: 0.7632\n",
            "Epoch: [39/100]---val, loss: 1.080678, acc: 0.7057\n",
            "Epoch: [39/100]---tar, loss: 1.080678, acc: 0.7057\n",
            "\n",
            "Epoch: [40/100]---src, loss: 1.052534, acc: 0.7629\n",
            "Epoch: [40/100]---val, loss: 1.089370, acc: 0.7233\n",
            "Epoch: [40/100]---tar, loss: 1.089370, acc: 0.7233\n",
            "\n",
            "Epoch: [41/100]---src, loss: 1.030919, acc: 0.7696\n",
            "Epoch: [41/100]---val, loss: 1.074973, acc: 0.7195\n",
            "Epoch: [41/100]---tar, loss: 1.074973, acc: 0.7195\n",
            "\n",
            "Epoch: [42/100]---src, loss: 1.037529, acc: 0.7632\n",
            "Epoch: [42/100]---val, loss: 1.025179, acc: 0.7296\n",
            "Epoch: [42/100]---tar, loss: 1.025179, acc: 0.7296\n",
            "\n",
            "Epoch: [43/100]---src, loss: 1.006569, acc: 0.7685\n",
            "Epoch: [43/100]---val, loss: 1.043264, acc: 0.7233\n",
            "Epoch: [43/100]---tar, loss: 1.043264, acc: 0.7233\n",
            "\n",
            "Epoch: [44/100]---src, loss: 0.998667, acc: 0.7675\n",
            "Epoch: [44/100]---val, loss: 1.038273, acc: 0.7145\n",
            "Epoch: [44/100]---tar, loss: 1.038273, acc: 0.7145\n",
            "\n",
            "Epoch: [45/100]---src, loss: 1.020539, acc: 0.7661\n",
            "Epoch: [45/100]---val, loss: 1.009193, acc: 0.7195\n",
            "Epoch: [45/100]---tar, loss: 1.009193, acc: 0.7195\n",
            "\n",
            "Epoch: [46/100]---src, loss: 0.972558, acc: 0.7707\n",
            "Epoch: [46/100]---val, loss: 1.006032, acc: 0.7283\n",
            "Epoch: [46/100]---tar, loss: 1.006032, acc: 0.7283\n",
            "\n",
            "Epoch: [47/100]---src, loss: 0.944310, acc: 0.7870\n",
            "Epoch: [47/100]---val, loss: 1.001397, acc: 0.7321\n",
            "Epoch: [47/100]---tar, loss: 1.001397, acc: 0.7321\n",
            "\n",
            "Epoch: [48/100]---src, loss: 0.939340, acc: 0.7867\n",
            "Epoch: [48/100]---val, loss: 1.010252, acc: 0.7208\n",
            "Epoch: [48/100]---tar, loss: 1.010252, acc: 0.7208\n",
            "\n",
            "Epoch: [49/100]---src, loss: 0.965853, acc: 0.7700\n",
            "Epoch: [49/100]---val, loss: 0.990455, acc: 0.7270\n",
            "Epoch: [49/100]---tar, loss: 0.990455, acc: 0.7270\n",
            "\n",
            "Epoch: [50/100]---src, loss: 0.945031, acc: 0.7796\n",
            "Epoch: [50/100]---val, loss: 0.976124, acc: 0.7384\n",
            "Epoch: [50/100]---tar, loss: 0.976124, acc: 0.7384\n",
            "\n",
            "Epoch: [51/100]---src, loss: 0.925800, acc: 0.7796\n",
            "Epoch: [51/100]---val, loss: 0.981626, acc: 0.7358\n",
            "Epoch: [51/100]---tar, loss: 0.981626, acc: 0.7358\n",
            "\n",
            "Epoch: [52/100]---src, loss: 0.910882, acc: 0.7856\n",
            "Epoch: [52/100]---val, loss: 0.979047, acc: 0.7283\n",
            "Epoch: [52/100]---tar, loss: 0.979047, acc: 0.7283\n",
            "\n",
            "Epoch: [53/100]---src, loss: 0.916404, acc: 0.7810\n",
            "Epoch: [53/100]---val, loss: 0.978938, acc: 0.7270\n",
            "Epoch: [53/100]---tar, loss: 0.978938, acc: 0.7270\n",
            "\n",
            "Epoch: [54/100]---src, loss: 0.911034, acc: 0.7810\n",
            "Epoch: [54/100]---val, loss: 0.969804, acc: 0.7182\n",
            "Epoch: [54/100]---tar, loss: 0.969804, acc: 0.7182\n",
            "\n",
            "Epoch: [55/100]---src, loss: 0.915695, acc: 0.7796\n",
            "Epoch: [55/100]---val, loss: 0.961963, acc: 0.7245\n",
            "Epoch: [55/100]---tar, loss: 0.961963, acc: 0.7245\n",
            "\n",
            "Epoch: [56/100]---src, loss: 0.895921, acc: 0.7813\n",
            "Epoch: [56/100]---val, loss: 0.962785, acc: 0.7208\n",
            "Epoch: [56/100]---tar, loss: 0.962785, acc: 0.7208\n",
            "\n",
            "Epoch: [57/100]---src, loss: 0.906429, acc: 0.7845\n",
            "Epoch: [57/100]---val, loss: 0.921835, acc: 0.7447\n",
            "Epoch: [57/100]---tar, loss: 0.921835, acc: 0.7447\n",
            "\n",
            "Epoch: [58/100]---src, loss: 0.872357, acc: 0.7898\n",
            "Epoch: [58/100]---val, loss: 0.936170, acc: 0.7384\n",
            "Epoch: [58/100]---tar, loss: 0.936170, acc: 0.7384\n",
            "\n",
            "Epoch: [59/100]---src, loss: 0.851871, acc: 0.7856\n",
            "Epoch: [59/100]---val, loss: 0.946391, acc: 0.7384\n",
            "Epoch: [59/100]---tar, loss: 0.946391, acc: 0.7384\n",
            "\n",
            "Epoch: [60/100]---src, loss: 0.855156, acc: 0.7913\n",
            "Epoch: [60/100]---val, loss: 0.940898, acc: 0.7208\n",
            "Epoch: [60/100]---tar, loss: 0.940898, acc: 0.7208\n",
            "\n",
            "Epoch: [61/100]---src, loss: 0.856642, acc: 0.7916\n",
            "Epoch: [61/100]---val, loss: 0.912621, acc: 0.7346\n",
            "Epoch: [61/100]---tar, loss: 0.912621, acc: 0.7346\n",
            "\n",
            "Epoch: [62/100]---src, loss: 0.844884, acc: 0.7952\n",
            "Epoch: [62/100]---val, loss: 0.968494, acc: 0.7145\n",
            "Epoch: [62/100]---tar, loss: 0.968494, acc: 0.7145\n",
            "\n",
            "Epoch: [63/100]---src, loss: 0.848652, acc: 0.7973\n",
            "Epoch: [63/100]---val, loss: 0.917978, acc: 0.7308\n",
            "Epoch: [63/100]---tar, loss: 0.917978, acc: 0.7308\n",
            "\n",
            "Epoch: [64/100]---src, loss: 0.814799, acc: 0.8040\n",
            "Epoch: [64/100]---val, loss: 0.885300, acc: 0.7283\n",
            "Epoch: [64/100]---tar, loss: 0.885300, acc: 0.7283\n",
            "\n",
            "Epoch: [65/100]---src, loss: 0.814652, acc: 0.8030\n",
            "Epoch: [65/100]---val, loss: 0.889700, acc: 0.7358\n",
            "Epoch: [65/100]---tar, loss: 0.889700, acc: 0.7358\n",
            "\n",
            "Epoch: [66/100]---src, loss: 0.804590, acc: 0.7984\n",
            "Epoch: [66/100]---val, loss: 0.904433, acc: 0.7308\n",
            "Epoch: [66/100]---tar, loss: 0.904433, acc: 0.7308\n",
            "\n",
            "Epoch: [67/100]---src, loss: 0.829857, acc: 0.7962\n",
            "Epoch: [67/100]---val, loss: 0.898638, acc: 0.7308\n",
            "Epoch: [67/100]---tar, loss: 0.898638, acc: 0.7308\n",
            "\n",
            "Epoch: [68/100]---src, loss: 0.829087, acc: 0.7977\n",
            "Epoch: [68/100]---val, loss: 0.868801, acc: 0.7245\n",
            "Epoch: [68/100]---tar, loss: 0.868801, acc: 0.7245\n",
            "\n",
            "Epoch: [69/100]---src, loss: 0.806952, acc: 0.8009\n",
            "Epoch: [69/100]---val, loss: 0.882487, acc: 0.7283\n",
            "Epoch: [69/100]---tar, loss: 0.882487, acc: 0.7283\n",
            "\n",
            "Epoch: [70/100]---src, loss: 0.814897, acc: 0.7966\n",
            "Epoch: [70/100]---val, loss: 0.885984, acc: 0.7333\n",
            "Epoch: [70/100]---tar, loss: 0.885984, acc: 0.7333\n",
            "\n",
            "Epoch: [71/100]---src, loss: 0.817298, acc: 0.7938\n",
            "Epoch: [71/100]---val, loss: 0.890698, acc: 0.7421\n",
            "Epoch: [71/100]---tar, loss: 0.890698, acc: 0.7421\n",
            "\n",
            "Epoch: [72/100]---src, loss: 0.801263, acc: 0.7994\n",
            "Epoch: [72/100]---val, loss: 0.876379, acc: 0.7270\n",
            "Epoch: [72/100]---tar, loss: 0.876379, acc: 0.7270\n",
            "\n",
            "Epoch: [73/100]---src, loss: 0.792241, acc: 0.8023\n",
            "Epoch: [73/100]---val, loss: 0.874526, acc: 0.7409\n",
            "Epoch: [73/100]---tar, loss: 0.874526, acc: 0.7409\n",
            "\n",
            "Epoch: [74/100]---src, loss: 0.796377, acc: 0.7977\n",
            "Epoch: [74/100]---val, loss: 0.896951, acc: 0.7296\n",
            "Epoch: [74/100]---tar, loss: 0.896951, acc: 0.7296\n",
            "\n",
            "Epoch: [75/100]---src, loss: 0.778059, acc: 0.8094\n",
            "Epoch: [75/100]---val, loss: 0.829124, acc: 0.7434\n",
            "Epoch: [75/100]---tar, loss: 0.829124, acc: 0.7434\n",
            "\n",
            "Epoch: [76/100]---src, loss: 0.788817, acc: 0.8001\n",
            "Epoch: [76/100]---val, loss: 0.868723, acc: 0.7346\n",
            "Epoch: [76/100]---tar, loss: 0.868723, acc: 0.7346\n",
            "\n",
            "Epoch: [77/100]---src, loss: 0.769491, acc: 0.8062\n",
            "Epoch: [77/100]---val, loss: 0.894442, acc: 0.7157\n",
            "Epoch: [77/100]---tar, loss: 0.894442, acc: 0.7157\n",
            "Training complete in 11m 45s\n"
          ]
        }
      ],
      "source": [
        "# TODO: fine-tuning function 호출하여 학습 진행\n",
        "finetune(model, dataloaders, optimizer = optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GR5Y1x4btm4y"
      },
      "outputs": [],
      "source": [
        "def test(model, target_test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    len_target_dataset = len(target_test_loader.dataset)\n",
        "    with torch.no_grad():\n",
        "        for data, target in target_test_loader:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            s_output = model.predict(data)\n",
        "            pred = torch.max(s_output, 1)[1]\n",
        "            correct += torch.sum(pred == target)\n",
        "    acc = correct.double() / len(target_test_loader.dataset)\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1O2CBxmxtm4y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.7446540880503144\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('model.pkl'))\n",
        "acc_test = test(model, dataloaders['tar'])\n",
        "print(f'Test accuracy: {acc_test}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7Iuk_Mitm4z"
      },
      "source": [
        "여기까지가 fine-tuning 파트입니다. 실제 학습에서는 learning rate decay 같은 기법도 사용하지만, 이 과제에서는 그것이 핵심이 아니므로 생략합니다.\n",
        "\n",
        "이제 같은 dataloader를 그대로 활용해서 domain adaptation 실험을 이어가봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO4c_QcGtm4z"
      },
      "source": [
        "# Domain Adaptation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJwcwLQftm40"
      },
      "source": [
        "Domain adaptation의 핵심 구조는 fine-tuning과 매우 비슷하지만,\n",
        "두 도메인 간 분포 차이를 줄이기 위한 loss function을 추가해야 합니다.\n",
        "\n",
        "여기서는 MMD와 Coral loss를 사용해 두 도메인 간 분포 차이를 계산하는 loss function을 정의합니다. \n",
        "\n",
        "해당 loss를 이용할 수 있도록 새로운 모델 클래스를 정의하고 source의 feature와 label, 그리고 target feature를 모두 이용하도록 학습 스크립트를 수정해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy_1xwdJtm40"
      },
      "source": [
        "### Loss function\n",
        "Domain Adaptation에서 가장 많이 사용되는 손실 함수는 MMD (Maximum Mean Discrepancy)입니다.\n",
        "\n",
        "비교를 위해 또 다른 대표적인 손실 함수인 CORAL (CORrelation ALignment)도 함께 살펴봅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3-wKorUtm40"
      },
      "source": [
        "#### MMD loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MpQH6VFwtm41"
      },
      "outputs": [],
      "source": [
        "class MMD_loss(nn.Module):\n",
        "    def __init__(self, kernel_type='rbf', kernel_mul=2.0, kernel_num=5):\n",
        "        super(MMD_loss, self).__init__()\n",
        "        self.kernel_num = kernel_num\n",
        "        self.kernel_mul = kernel_mul\n",
        "        self.fix_sigma = None\n",
        "        self.kernel_type = kernel_type\n",
        "\n",
        "    def guassian_kernel(self, source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
        "        n_samples = int(source.size()[0]) + int(target.size()[0])\n",
        "        total = torch.cat([source, target], dim=0)\n",
        "        total0 = total.unsqueeze(0).expand(\n",
        "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
        "        total1 = total.unsqueeze(1).expand(\n",
        "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
        "        L2_distance = ((total0-total1)**2).sum(2)\n",
        "        if fix_sigma:\n",
        "            bandwidth = fix_sigma\n",
        "        else:\n",
        "            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
        "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
        "        bandwidth_list = [bandwidth * (kernel_mul**i)\n",
        "                          for i in range(kernel_num)]\n",
        "        kernel_val = [torch.exp(-L2_distance / bandwidth_temp)\n",
        "                      for bandwidth_temp in bandwidth_list]\n",
        "        return sum(kernel_val)\n",
        "\n",
        "    def linear_mmd2(self, f_of_X, f_of_Y):\n",
        "        loss = 0.0\n",
        "        delta = f_of_X.float().mean(0) - f_of_Y.float().mean(0)\n",
        "        loss = delta.dot(delta.T)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, source, target):\n",
        "        if self.kernel_type == 'linear':\n",
        "            return self.linear_mmd2(source, target)\n",
        "        elif self.kernel_type == 'rbf':\n",
        "            batch_size = int(source.size()[0])\n",
        "            kernels = self.guassian_kernel(\n",
        "                source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n",
        "            XX = torch.mean(kernels[:batch_size, :batch_size])\n",
        "            YY = torch.mean(kernels[batch_size:, batch_size:])\n",
        "            XY = torch.mean(kernels[:batch_size, batch_size:])\n",
        "            YX = torch.mean(kernels[batch_size:, :batch_size])\n",
        "            loss = torch.mean(XX + YY - XY - YX)\n",
        "            return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcfUy_2Dtm41"
      },
      "source": [
        "#### CORAL loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "uZhKJq15tm41"
      },
      "outputs": [],
      "source": [
        "def CORAL(source, target):\n",
        "    d = source.size(1)\n",
        "    ns, nt = source.size(0), target.size(0)\n",
        "\n",
        "    # source covariance\n",
        "    tmp_s = torch.ones((1, ns)).cuda() @ source\n",
        "    cs = (source.t() @ source - (tmp_s.t() @ tmp_s) / ns) / (ns - 1)\n",
        "\n",
        "    # target covariance\n",
        "    tmp_t = torch.ones((1, nt)).cuda() @ target\n",
        "    ct = (target.t() @ target - (tmp_t.t() @ tmp_t) / nt) / (nt - 1)\n",
        "\n",
        "    # frobenius norm\n",
        "    loss = (cs - ct).pow(2).sum().sqrt()\n",
        "    loss = loss / (4 * d * d)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB2cDp8Gtm41"
      },
      "source": [
        "### Model\n",
        "여기서도 backbone으로는 ResNet-50을 사용합니다.\n",
        "다만 이번에는 ResNet-50의 마지막 classifier layer를 제거한 feature extractor로 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UOLx_OSxtm41"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "class ResNet50Fc(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet50Fc, self).__init__()\n",
        "        model_resnet50 = models.resnet50(pretrained=True)\n",
        "        self.conv1 = model_resnet50.conv1\n",
        "        self.bn1 = model_resnet50.bn1\n",
        "        self.relu = model_resnet50.relu\n",
        "        self.maxpool = model_resnet50.maxpool\n",
        "        self.layer1 = model_resnet50.layer1\n",
        "        self.layer2 = model_resnet50.layer2\n",
        "        self.layer3 = model_resnet50.layer3\n",
        "        self.layer4 = model_resnet50.layer4\n",
        "        self.avgpool = model_resnet50.avgpool\n",
        "        self.__in_features = model_resnet50.fc.in_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "    def output_num(self):\n",
        "        return self.__in_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRgdNM3Wtm42"
      },
      "source": [
        "이제 Domain Adaptation을 위한 핵심 모델 클래스를 정의합니다.\n",
        "\n",
        "ResNet-50을 기반으로 하되, bottleneck layer와 새로운 fc layer를 추가합니다.\n",
        "\n",
        "중요한 점은 adapt_loss 함수로 우리가 정의한 MMD 또는 CORAL loss를 forward pass에서 함께 계산한다는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "oC5NKJpJtm42"
      },
      "outputs": [],
      "source": [
        "class TransferNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_class, \n",
        "                 base_net='resnet50', \n",
        "                 transfer_loss='mmd', \n",
        "                 use_bottleneck=True, \n",
        "                 bottleneck_width=256, \n",
        "                 width=1024):\n",
        "        super(TransferNet, self).__init__()\n",
        "        if base_net == 'resnet50':\n",
        "            self.base_network = ResNet50Fc()\n",
        "            self.feature_dim = self.base_network.output_num()\n",
        "            # TODO: ResNet50 기반 feature extractor를 정의\n",
        "        else:\n",
        "            # Your own basenet\n",
        "            return\n",
        "        self.use_bottleneck = use_bottleneck\n",
        "        self.transfer_loss = transfer_loss\n",
        "        bottleneck_list = [\n",
        "                nn.Linear(self.feature_dim, bottleneck_width),\n",
        "                nn.BatchNorm1d(bottleneck_width),\n",
        "                nn.ReLU()\n",
        "            ] # TODO: bottleneck layer를 정의\n",
        "        self.bottleneck_layer = nn.Sequential(*bottleneck_list)\n",
        "        self.output_dim = width * 2\n",
        "        classifier_layer_list = [\n",
        "            nn.Linear(self.output_dim, width),\n",
        "            nn.BatchNorm1d(width),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(width, width),\n",
        "            nn.BatchNorm1d(width),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(width, num_class)\n",
        "        ]# TODO: classifier layer를 정의\n",
        "        self.classifier_layer = nn.Sequential(*classifier_layer_list)\n",
        "\n",
        "        self.bottleneck_layer[0].weight.data.normal_(0, 0.005)\n",
        "        self.bottleneck_layer[0].bias.data.fill_(0.1)\n",
        "        for i in range(2):\n",
        "            self.classifier_layer[i * 3].weight.data.normal_(0, 0.01)\n",
        "            self.classifier_layer[i * 3].bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, source, target):\n",
        "        source = self.base_network(source)\n",
        "        target = self.base_network(target)\n",
        "        source_clf = self.classifier_layer(source)\n",
        "        if self.use_bottleneck:\n",
        "            source = self.bottleneck_layer(source)\n",
        "            target = self.bottleneck_layer(target)\n",
        "        transfer_loss = self.adapt_loss(source, target, self.transfer_loss)\n",
        "        return source_clf, transfer_loss\n",
        "\n",
        "    def predict(self, x):\n",
        "        features = self.base_network(x)\n",
        "        clf = self.classifier_layer(features)\n",
        "        return clf\n",
        "\n",
        "    def adapt_loss(self, X, Y, adapt_loss):\n",
        "        \"\"\"Compute adaptation loss, currently we support mmd and coral\n",
        "\n",
        "        Arguments:\n",
        "            X {tensor} -- source matrix\n",
        "            Y {tensor} -- target matrix\n",
        "            adapt_loss {string} -- loss type, 'mmd' or 'coral'. You can add your own loss\n",
        "\n",
        "        Returns:\n",
        "            [tensor] -- adaptation loss tensor\n",
        "        \"\"\"\n",
        "        if adapt_loss == 'mmd':\n",
        "            mmd_loss = MMD_loss()\n",
        "            loss = mmd_loss(X, Y)\n",
        "        elif adapt_loss == 'coral':\n",
        "            loss = CORAL(X, Y)\n",
        "        else:\n",
        "            # Your own loss\n",
        "            loss = 0\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAdPs27btm42"
      },
      "source": [
        "### Train\n",
        "이제 Domain Adaptation 모델을 학습시켜 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "OK6P8uMDtm42"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/homework/homeowkr/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/root/homework/homeowkr/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "transfer_loss = 'mmd'\n",
        "learning_rate = 0.0001\n",
        "transfer_model = TransferNet(n_class, transfer_loss=transfer_loss, base_net='resnet50').cuda()\n",
        "optimizer = torch.optim.SGD([\n",
        "    {'params': transfer_model.base_network.parameters()},\n",
        "    {'params': transfer_model.bottleneck_layer.parameters(), 'lr': 10 * learning_rate},\n",
        "    {'params': transfer_model.classifier_layer.parameters(), 'lr': 10 * learning_rate},\n",
        "], lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
        "lamb = 10 # weight for transfer loss, it is a hyperparameter that needs to be tuned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WpUfcHItm42"
      },
      "source": [
        "학습 함수에서는 source 데이터와 label, target 데이터를 모두 사용해야 하므로, source와 target의 dataloader를 zip으로 묶어서 동시에 iterate합니다.\n",
        "\n",
        "보통 두 도메인의 샘플 수는 다르기 때문에, 여러 epoch에 걸쳐 무작위로 잘 섞이면 전체 데이터를 충분히 학습할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "AGlVNI2ktm42"
      },
      "outputs": [],
      "source": [
        "def train(dataloaders, model, optimizer):\n",
        "    source_loader, target_train_loader, target_test_loader = dataloaders['src'], dataloaders['val'], dataloaders['tar']\n",
        "    len_source_loader = len(source_loader)\n",
        "    len_target_loader = len(target_train_loader)\n",
        "    best_acc = 0\n",
        "    stop = 0\n",
        "    n_batch = min(len_source_loader, len_target_loader)\n",
        "    for e in range(n_epoch):\n",
        "        stop += 1\n",
        "        train_loss_clf, train_loss_transfer, train_loss_total = 0, 0, 0\n",
        "        model.train()\n",
        "        for (src, tar) in zip(source_loader, target_train_loader):\n",
        "            data_source, label_source = src\n",
        "            data_target, _ = tar\n",
        "            data_source, label_source = data_source.cuda(), label_source.cuda()\n",
        "            data_target = data_target.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            label_source_pred, transfer_loss = model(data_source, data_target)\n",
        "            clf_loss = criterion(label_source_pred, label_source)\n",
        "            loss = clf_loss + lamb * transfer_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss_clf = clf_loss.detach().item() + train_loss_clf\n",
        "            train_loss_transfer = transfer_loss.detach().item() + train_loss_transfer\n",
        "            train_loss_total = loss.detach().item() + train_loss_total\n",
        "        acc = test(model, target_test_loader)\n",
        "        print(f'Epoch: [{e:2d}/{n_epoch}], cls_loss: {train_loss_clf/n_batch:.4f}, transfer_loss: {train_loss_transfer/n_batch:.4f}, total_Loss: {train_loss_total/n_batch:.4f}, acc: {acc:.4f}')\n",
        "        if best_acc < acc:\n",
        "            best_acc = acc\n",
        "            torch.save(model.state_dict(), 'trans_model.pkl')\n",
        "            stop = 0\n",
        "        if stop >= early_stop:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "nqSCG6-Xtm43",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [ 0/100], cls_loss: 2.9569, transfer_loss: 0.1312, total_Loss: 4.2685, acc: 0.4881\n",
            "Epoch: [ 1/100], cls_loss: 1.8390, transfer_loss: 0.1238, total_Loss: 3.0770, acc: 0.6604\n",
            "Epoch: [ 2/100], cls_loss: 1.3906, transfer_loss: 0.1200, total_Loss: 2.5902, acc: 0.6843\n",
            "Epoch: [ 3/100], cls_loss: 1.1173, transfer_loss: 0.1190, total_Loss: 2.3075, acc: 0.7358\n",
            "Epoch: [ 4/100], cls_loss: 1.0321, transfer_loss: 0.1173, total_Loss: 2.2050, acc: 0.7296\n",
            "Epoch: [ 5/100], cls_loss: 0.9057, transfer_loss: 0.1065, total_Loss: 1.9708, acc: 0.7409\n",
            "Epoch: [ 6/100], cls_loss: 0.8431, transfer_loss: 0.0883, total_Loss: 1.7261, acc: 0.7346\n",
            "Epoch: [ 7/100], cls_loss: 0.8312, transfer_loss: 0.0846, total_Loss: 1.6775, acc: 0.7233\n",
            "Epoch: [ 8/100], cls_loss: 0.8579, transfer_loss: 0.0858, total_Loss: 1.7157, acc: 0.7145\n",
            "Epoch: [ 9/100], cls_loss: 0.8323, transfer_loss: 0.0717, total_Loss: 1.5491, acc: 0.7119\n",
            "Epoch: [10/100], cls_loss: 0.7168, transfer_loss: 0.0652, total_Loss: 1.3686, acc: 0.7044\n",
            "Epoch: [11/100], cls_loss: 0.7358, transfer_loss: 0.0755, total_Loss: 1.4911, acc: 0.7006\n",
            "Epoch: [12/100], cls_loss: 0.7074, transfer_loss: 0.0796, total_Loss: 1.5034, acc: 0.7220\n",
            "Epoch: [13/100], cls_loss: 0.6905, transfer_loss: 0.0737, total_Loss: 1.4278, acc: 0.6943\n",
            "Epoch: [14/100], cls_loss: 0.7409, transfer_loss: 0.0555, total_Loss: 1.2961, acc: 0.7044\n",
            "Epoch: [15/100], cls_loss: 0.6414, transfer_loss: 0.0699, total_Loss: 1.3399, acc: 0.7145\n",
            "Epoch: [16/100], cls_loss: 0.6483, transfer_loss: 0.0609, total_Loss: 1.2574, acc: 0.6994\n",
            "Epoch: [17/100], cls_loss: 0.6182, transfer_loss: 0.0555, total_Loss: 1.1731, acc: 0.7031\n",
            "Epoch: [18/100], cls_loss: 0.5742, transfer_loss: 0.0753, total_Loss: 1.3267, acc: 0.7006\n",
            "Epoch: [19/100], cls_loss: 0.6240, transfer_loss: 0.0721, total_Loss: 1.3446, acc: 0.7019\n",
            "Epoch: [20/100], cls_loss: 0.6017, transfer_loss: 0.0702, total_Loss: 1.3042, acc: 0.7157\n",
            "Epoch: [21/100], cls_loss: 0.6237, transfer_loss: 0.0621, total_Loss: 1.2450, acc: 0.7421\n",
            "Epoch: [22/100], cls_loss: 0.5936, transfer_loss: 0.0527, total_Loss: 1.1209, acc: 0.7119\n",
            "Epoch: [23/100], cls_loss: 0.5750, transfer_loss: 0.0659, total_Loss: 1.2342, acc: 0.6717\n",
            "Epoch: [24/100], cls_loss: 0.6235, transfer_loss: 0.0578, total_Loss: 1.2015, acc: 0.6742\n",
            "Epoch: [25/100], cls_loss: 0.5095, transfer_loss: 0.0538, total_Loss: 1.0480, acc: 0.6818\n",
            "Epoch: [26/100], cls_loss: 0.5694, transfer_loss: 0.0427, total_Loss: 0.9961, acc: 0.7321\n",
            "Epoch: [27/100], cls_loss: 0.5222, transfer_loss: 0.0601, total_Loss: 1.1228, acc: 0.6906\n",
            "Epoch: [28/100], cls_loss: 0.4975, transfer_loss: 0.0588, total_Loss: 1.0850, acc: 0.7006\n",
            "Epoch: [29/100], cls_loss: 0.4801, transfer_loss: 0.0554, total_Loss: 1.0338, acc: 0.6868\n",
            "Epoch: [30/100], cls_loss: 0.5307, transfer_loss: 0.0492, total_Loss: 1.0227, acc: 0.7233\n",
            "Epoch: [31/100], cls_loss: 0.4511, transfer_loss: 0.0534, total_Loss: 0.9855, acc: 0.7233\n",
            "Epoch: [32/100], cls_loss: 0.4421, transfer_loss: 0.0552, total_Loss: 0.9937, acc: 0.7119\n",
            "Epoch: [33/100], cls_loss: 0.4077, transfer_loss: 0.0611, total_Loss: 1.0191, acc: 0.7145\n",
            "Epoch: [34/100], cls_loss: 0.3695, transfer_loss: 0.0483, total_Loss: 0.8528, acc: 0.6855\n",
            "Epoch: [35/100], cls_loss: 0.4207, transfer_loss: 0.0378, total_Loss: 0.7987, acc: 0.7308\n",
            "Epoch: [36/100], cls_loss: 0.3951, transfer_loss: 0.0462, total_Loss: 0.8573, acc: 0.7170\n",
            "Epoch: [37/100], cls_loss: 0.4246, transfer_loss: 0.0521, total_Loss: 0.9452, acc: 0.7170\n",
            "Epoch: [38/100], cls_loss: 0.3821, transfer_loss: 0.0715, total_Loss: 1.0973, acc: 0.7182\n",
            "Epoch: [39/100], cls_loss: 0.4087, transfer_loss: 0.0518, total_Loss: 0.9267, acc: 0.6881\n",
            "Epoch: [40/100], cls_loss: 0.4178, transfer_loss: 0.0506, total_Loss: 0.9239, acc: 0.6969\n",
            "Epoch: [41/100], cls_loss: 0.4392, transfer_loss: 0.0389, total_Loss: 0.8281, acc: 0.6906\n"
          ]
        }
      ],
      "source": [
        "# TODO: Domain Adaptation 모델을 학습시키는 함수를 호출하여 학습 진행\n",
        "train(dataloaders=dataloaders, model= transfer_model, optimizer = optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "yWvYODRDtm43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.7421383647798742\n"
          ]
        }
      ],
      "source": [
        "transfer_model.load_state_dict(torch.load('trans_model.pkl'))\n",
        "acc_test = test(transfer_model, dataloaders['tar'])\n",
        "print(f'Test accuracy: {acc_test}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "deep_transfer_tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "homeowkr",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
